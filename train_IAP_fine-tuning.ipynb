{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import pytorch_lightning as pl\n",
    "import ML_library        as MLL\n",
    "import matplotlib.pyplot as plt\n",
    "import matgl\n",
    "import os\n",
    "import warnings\n",
    "import glob\n",
    "import torch\n",
    "\n",
    "from __future__                import annotations\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from matgl.ext.pymatgen        import Structure2Graph, get_element_list\n",
    "from matgl.graph.data          import M3GNetDataset, MGLDataLoader, collate_fn_efs\n",
    "from matgl.utils.training      import PotentialLightningModule\n",
    "\n",
    "# To suppress warnings for clearer output\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce15143c5a02981e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "device"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bdd19b6f63c722e4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_train_path = 'm3gnet_dataset.xlsx'\n",
    "model_load_path = 'M3GNet-MP-2021.2.8-PES'\n",
    "model_save_path = 'finetuned_model'\n",
    "\n",
    "# Whether to include charge or not\n",
    "charged = True\n",
    "\n",
    "# 0: material, 1: charge state, 2: ionic step\n",
    "depth = 1\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 128\n",
    "\n",
    "# Stress weight for training\n",
    "stress_weight = 0\n",
    "\n",
    "# Ratios for diving training data\n",
    "test_ratio       = 0.2\n",
    "validation_ratio = 0.2\n",
    "\n",
    "# Number of epoch for re-training\n",
    "max_epochs = 10\n",
    "\n",
    "# Learning-rate for re-training\n",
    "lr = 1e-4\n",
    "\n",
    "dpi = 100\n",
    "\n",
    "# Version of training you specifically want to analyze\n",
    "current_version = 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8205eacc93a87700"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load simulation data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8531072de38c77ad"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Each folder names a new column, and structure, energy, forces and stresses\n",
    "# of each ionic step are loaded\n",
    "\n",
    "if os.path.exists(data_train_path):\n",
    "    # Load data for model training\n",
    "    m3gnet_dataset = pd.read_excel(data_train_path, index_col=0, header=[0,1,2])\n",
    "else:\n",
    "    # Path to dataset, structured as:\n",
    "    # path_to_dataset\n",
    "    #     material_i\n",
    "    #         defect_i\n",
    "    #             simulation_i (containing vasprun.xml)\n",
    "    path_to_dataset = '../../../../Desktop/defects'\n",
    "\n",
    "    # Extract the data\n",
    "    source_m3gnet_dataset = MLL.extract_vaspruns_dataset(path_to_dataset, charged=charged)\n",
    "    #source_m3gnet_dataset.to_excel(data_train_path)\n",
    "\n",
    "source_m3gnet_dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88c86b5aee65667b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "len(source_m3gnet_dataset)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fdaa03ef6b919c1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Split data into train-validation-test sets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1665e69cec7b6e4e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Decide if we split in terms of mateiral, defect state or simulation directly"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2a4ca22ac0cab57"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Clone (copy) the DataFrame\n",
    "m3gnet_dataset = source_m3gnet_dataset.copy()\n",
    "\n",
    "# Remove the outer (top-level) column index up to depth-1 level\n",
    "for i in range(depth):\n",
    "    m3gnet_dataset.columns = m3gnet_dataset.columns.droplevel(0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebcae6e798ff6059"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "m3gnet_dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2292e14517e0ad3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Splitting into train-validation-test sets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86b2b102ec3e5805"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Check if data has been already split, else do it randomly\n",
    "\n",
    "path_to_test_labels       = 'test_labels.txt'\n",
    "path_to_validation_labels = 'validation_labels.txt'\n",
    "path_to_train_labels      = 'train_labels.txt'\n",
    "\n",
    "if os.path.exists(path_to_test_labels) and os.path.exists(path_to_validation_labels) and os.path.exists(path_to_train_labels):\n",
    "    # Read labels splitting (which are strings)\n",
    "    test_labels       = np.genfromtxt(path_to_test_labels,       dtype='str').tolist()\n",
    "    validation_labels = np.genfromtxt(path_to_validation_labels, dtype='str').tolist()\n",
    "    train_labels      = np.genfromtxt(path_to_train_labels,      dtype='str').tolist()\n",
    "else:\n",
    "    # Define unique labels, wrt the outer column\n",
    "    unique_labels = np.unique(m3gnet_dataset.columns.get_level_values(0))\n",
    "\n",
    "    # Shuffle the list of unique labels\n",
    "    np.random.shuffle(unique_labels)\n",
    "\n",
    "    # Define the sizes of every set\n",
    "    # Corresponds to the size wrt the number of unique materials in the dataset\n",
    "    test_size       = int(test_ratio       * len(unique_labels))\n",
    "    validation_size = int(validation_ratio * len(unique_labels))\n",
    "\n",
    "    test_labels       = unique_labels[:test_size]\n",
    "    validation_labels = unique_labels[test_size:test_size+validation_size]\n",
    "    train_labels      = unique_labels[test_size+validation_size:]\n",
    "    \n",
    "    # Save this splitting for transfer-learning approaches\n",
    "    np.savetxt(path_to_test_labels,       test_labels,       fmt='%s')\n",
    "    np.savetxt(path_to_validation_labels, validation_labels, fmt='%s')\n",
    "    np.savetxt(path_to_train_labels,      train_labels,      fmt='%s')\n",
    "\n",
    "# Use the loaded/computed labels to generate split datasets\n",
    "test_dataset       = m3gnet_dataset[test_labels]\n",
    "validation_dataset = m3gnet_dataset[validation_labels]\n",
    "train_dataset      = m3gnet_dataset[train_labels]\n",
    "\n",
    "n_test       = np.shape(test_dataset)[1]\n",
    "n_validation = np.shape(validation_dataset)[1]\n",
    "n_train      = np.shape(train_dataset)[1]\n",
    "\n",
    "print(f'Using {n_train} samples to train, {n_validation} to evaluate, and {n_test} to test')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bdde103a45a6c1ed"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Convert into graph database"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66e4970ac31246c2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for i in range(len(train_dataset.loc['force'].values)):\n",
    "    train_dataset.loc['force'].values[i] = train_dataset.loc['force'].values[i].tolist()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab03a7629fbbc90f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for i in range(len(validation_dataset.loc['force'].values)):\n",
    "    validation_dataset.loc['force'].values[i] = validation_dataset.loc['force'].values[i].tolist()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4405c616a4d8ed3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for i in range(len(test_dataset.loc['force'].values)):\n",
    "    test_dataset.loc['force'].values[i] = test_dataset.loc['force'].values[i].tolist()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9315b2490e8d5069"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "all_data = []\n",
    "for i in range(3):  # Iterate over train-validation-test sets\n",
    "    name    = ['train', 'val', 'test'][i]\n",
    "    dataset = [train_dataset, validation_dataset, test_dataset][i]\n",
    "    \n",
    "    # Extract data from dataset\n",
    "    structures    = dataset.loc['structure'].values.tolist()\n",
    "    element_types = get_element_list(structures)\n",
    "    converter     = Structure2Graph(element_types=element_types, cutoff=5.0)\n",
    "    \n",
    "    # Define data labels from dataset\n",
    "    stresses = dataset.loc['stress'].values.tolist()\n",
    "    if stress_weight == 0:\n",
    "        stresses = [np.zeros((3, 3)).tolist() for s in structures]\n",
    "\n",
    "    labels = {\n",
    "        'energies': dataset.loc['energy'].values.tolist(),\n",
    "        'forces':   dataset.loc['force'].values.tolist(),\n",
    "        'stresses': stresses,\n",
    "    }\n",
    "    \n",
    "    # Generate dataset\n",
    "    data = M3GNetDataset(\n",
    "        filename=f'dgl_graph-{name}.bin',\n",
    "        filename_line_graph=f'dgl_line_graph-{name}.bin',\n",
    "        filename_state_attr=f'state_attr-{name}.pt',\n",
    "        filename_labels=f'labels-{name}.json',\n",
    "        threebody_cutoff=4.0,\n",
    "        structures=structures,\n",
    "        converter=converter,\n",
    "        labels=labels,\n",
    "        name=f'M3GNetDataset-{name}',\n",
    "    )\n",
    "    all_data.append(data)\n",
    "\n",
    "train_data, val_data, test_data = all_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99448a3c9aa48ee1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = MGLDataLoader(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    collate_fn=collate_fn_efs,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "679eb23293e5a538"
  },
  {
   "cell_type": "markdown",
   "id": "f9fb577b",
   "metadata": {},
   "source": [
    "# Retrain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7fac981a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T16:18:55.703532196Z",
     "start_time": "2024-02-26T16:18:55.675112850Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download a pre-trained M3GNet\n",
    "m3gnet_nnp       = matgl.load_model(model_load_path)\n",
    "model_pretrained = m3gnet_nnp.model\n",
    "\n",
    "# Stress and site-wise are added to training loss\n",
    "# Stresses are being computed (calc_stress=True)\n",
    "lit_module_finetune = PotentialLightningModule(model=model_pretrained,\n",
    "                                               stress_weight=stress_weight,\n",
    "                                               loss='mse_loss',\n",
    "                                               lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2a3ffedd",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-02-26T17:04:41.112730271Z",
     "start_time": "2024-02-26T17:04:40.778748896Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | mae   | MeanAbsoluteError | 0     \n",
      "1 | rmse  | MeanSquaredError  | 0     \n",
      "2 | model | Potential         | 288 K \n",
      "--------------------------------------------\n",
      "288 K     Trainable params\n",
      "0         Non-trainable params\n",
      "288 K     Total params\n",
      "1.153     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[41], line 11\u001B[0m\n\u001B[1;32m      3\u001B[0m logger  \u001B[38;5;241m=\u001B[39m CSVLogger(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlogs\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m      4\u001B[0m                     name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mM3GNet_finetuning\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      6\u001B[0m trainer \u001B[38;5;241m=\u001B[39m pl\u001B[38;5;241m.\u001B[39mTrainer(max_epochs\u001B[38;5;241m=\u001B[39mmax_epochs,\n\u001B[1;32m      7\u001B[0m                      accelerator\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgpu\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m      8\u001B[0m                      logger\u001B[38;5;241m=\u001B[39mlogger,\n\u001B[1;32m      9\u001B[0m                      inference_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m---> 11\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlit_module_finetune\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m            \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_loader\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m           \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# Save trained model\u001B[39;00m\n\u001B[1;32m     17\u001B[0m model_pretrained\u001B[38;5;241m.\u001B[39msave(model_save_path)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:544\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    542\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m=\u001B[39m TrainerStatus\u001B[38;5;241m.\u001B[39mRUNNING\n\u001B[1;32m    543\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m--> 544\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    545\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[1;32m    546\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:44\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[0;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m     42\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     43\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m---> 44\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n\u001B[1;32m     47\u001B[0m     _call_teardown_hook(trainer)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:580\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    573\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    574\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_select_ckpt_path(\n\u001B[1;32m    575\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[1;32m    576\u001B[0m     ckpt_path,\n\u001B[1;32m    577\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    578\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    579\u001B[0m )\n\u001B[0;32m--> 580\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    582\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n\u001B[1;32m    583\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:989\u001B[0m, in \u001B[0;36mTrainer._run\u001B[0;34m(self, model, ckpt_path)\u001B[0m\n\u001B[1;32m    984\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_signal_connector\u001B[38;5;241m.\u001B[39mregister_signal_handlers()\n\u001B[1;32m    986\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    987\u001B[0m \u001B[38;5;66;03m# RUN THE TRAINER\u001B[39;00m\n\u001B[1;32m    988\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m--> 989\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    991\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    992\u001B[0m \u001B[38;5;66;03m# POST-Training CLEAN UP\u001B[39;00m\n\u001B[1;32m    993\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    994\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: trainer tearing down\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1033\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1031\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining:\n\u001B[1;32m   1032\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m isolate_rng():\n\u001B[0;32m-> 1033\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_sanity_check\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1034\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n\u001B[1;32m   1035\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit_loop\u001B[38;5;241m.\u001B[39mrun()\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1062\u001B[0m, in \u001B[0;36mTrainer._run_sanity_check\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1059\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_callback_hooks(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_sanity_check_start\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1061\u001B[0m \u001B[38;5;66;03m# run eval step\u001B[39;00m\n\u001B[0;32m-> 1062\u001B[0m \u001B[43mval_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1064\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_callback_hooks(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_sanity_check_end\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1066\u001B[0m \u001B[38;5;66;03m# reset logger connector\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:182\u001B[0m, in \u001B[0;36m_no_grad_context.<locals>._decorator\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    180\u001B[0m     context_manager \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mno_grad\n\u001B[1;32m    181\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context_manager():\n\u001B[0;32m--> 182\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mloop_run\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:134\u001B[0m, in \u001B[0;36m_EvaluationLoop.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    132\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mis_last_batch \u001B[38;5;241m=\u001B[39m data_fetcher\u001B[38;5;241m.\u001B[39mdone\n\u001B[1;32m    133\u001B[0m     \u001B[38;5;66;03m# run step hooks\u001B[39;00m\n\u001B[0;32m--> 134\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_evaluation_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloader_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloader_iter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    135\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[1;32m    136\u001B[0m     \u001B[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001B[39;00m\n\u001B[1;32m    137\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:391\u001B[0m, in \u001B[0;36m_EvaluationLoop._evaluation_step\u001B[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001B[0m\n\u001B[1;32m    385\u001B[0m hook_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_step\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mtesting \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalidation_step\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    386\u001B[0m step_args \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    387\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001B[1;32m    388\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m using_dataloader_iter\n\u001B[1;32m    389\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m (dataloader_iter,)\n\u001B[1;32m    390\u001B[0m )\n\u001B[0;32m--> 391\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_strategy_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhook_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mstep_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    393\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mincrement_processed()\n\u001B[1;32m    395\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m using_dataloader_iter:\n\u001B[1;32m    396\u001B[0m     \u001B[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:309\u001B[0m, in \u001B[0;36m_call_strategy_hook\u001B[0;34m(trainer, hook_name, *args, **kwargs)\u001B[0m\n\u001B[1;32m    306\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    308\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[Strategy]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 309\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    311\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n\u001B[1;32m    312\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m prev_fx_name\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:403\u001B[0m, in \u001B[0;36mStrategy.validation_step\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    401\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module:\n\u001B[1;32m    402\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_redirection(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalidation_step\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 403\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlightning_module\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidation_step\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/matgl/utils/training.py:60\u001B[0m, in \u001B[0;36mMatglLightningModuleMixin.validation_step\u001B[0;34m(self, batch, batch_idx)\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mvalidation_step\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch: \u001B[38;5;28mtuple\u001B[39m, batch_idx: \u001B[38;5;28mint\u001B[39m):\n\u001B[1;32m     54\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Validation step.\u001B[39;00m\n\u001B[1;32m     55\u001B[0m \n\u001B[1;32m     56\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;124;03m        batch: Data batch.\u001B[39;00m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;124;03m        batch_idx: Batch index.\u001B[39;00m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 60\u001B[0m     results, batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog_dict(  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m     62\u001B[0m         {\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mval_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m: val \u001B[38;5;28;01mfor\u001B[39;00m key, val \u001B[38;5;129;01min\u001B[39;00m results\u001B[38;5;241m.\u001B[39mitems()},\n\u001B[1;32m     63\u001B[0m         batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     67\u001B[0m         sync_dist\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msync_dist,  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m     68\u001B[0m     )\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTotal_Loss\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/matgl/utils/training.py:380\u001B[0m, in \u001B[0;36mPotentialLightningModule.step\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m    378\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    379\u001B[0m     g, lat, l_g, state_attr, energies, forces, stresses \u001B[38;5;241m=\u001B[39m batch\n\u001B[0;32m--> 380\u001B[0m     e, f, s, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mg\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlat\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlat\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate_attr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstate_attr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ml_g\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43ml_g\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    381\u001B[0m     preds \u001B[38;5;241m=\u001B[39m (e, f, s)\n\u001B[1;32m    382\u001B[0m     labels \u001B[38;5;241m=\u001B[39m (energies, forces, stresses)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/matgl/utils/training.py:359\u001B[0m, in \u001B[0;36mPotentialLightningModule.forward\u001B[0;34m(self, g, lat, l_g, state_attr)\u001B[0m\n\u001B[1;32m    356\u001B[0m     e, f, s, h, m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(g\u001B[38;5;241m=\u001B[39mg, lat\u001B[38;5;241m=\u001B[39mlat, l_g\u001B[38;5;241m=\u001B[39ml_g, state_attr\u001B[38;5;241m=\u001B[39mstate_attr)\n\u001B[1;32m    357\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m e, f, s, h, m\n\u001B[0;32m--> 359\u001B[0m e, f, s, h \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mg\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlat\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlat\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ml_g\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43ml_g\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate_attr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstate_attr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    360\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m e, f, s, h\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/matgl/apps/pes.py:88\u001B[0m, in \u001B[0;36mPotential.forward\u001B[0;34m(self, g, lat, state_attr, l_g)\u001B[0m\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcalc_stresses:\n\u001B[1;32m     87\u001B[0m     st\u001B[38;5;241m.\u001B[39mrequires_grad_(\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m---> 88\u001B[0m lattice \u001B[38;5;241m=\u001B[39m lat \u001B[38;5;241m@\u001B[39m (\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meye\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mst\u001B[49m)\n\u001B[1;32m     89\u001B[0m g\u001B[38;5;241m.\u001B[39medata[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlattice\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrepeat_interleave(lattice, g\u001B[38;5;241m.\u001B[39mbatch_num_edges(), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     90\u001B[0m g\u001B[38;5;241m.\u001B[39medata[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpbc_offshift\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m (g\u001B[38;5;241m.\u001B[39medata[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpbc_offset\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39munsqueeze(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m*\u001B[39m g\u001B[38;5;241m.\u001B[39medata[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlattice\u001B[39m\u001B[38;5;124m\"\u001B[39m])\u001B[38;5;241m.\u001B[39msum(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "# If you wish to disable GPU or MPS (M1 mac) training, use the accelerator='cpu' kwarg.\n",
    "# accelerator='auto' selects the appropriate Accelerator\n",
    "logger  = CSVLogger('logs',\n",
    "                    name='M3GNet_finetuning')\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=max_epochs,\n",
    "                     accelerator='auto',\n",
    "                     logger=logger,\n",
    "                     inference_mode=False)\n",
    "\n",
    "trainer.fit(model=lit_module_finetune,\n",
    "            train_dataloaders=train_loader,\n",
    "            val_dataloaders=val_loader\n",
    "           )\n",
    "\n",
    "# Save trained model\n",
    "model_pretrained.save(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080f1dbc",
   "metadata": {},
   "source": [
    "# Analyze metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3f2256",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-02-26T15:43:41.310400348Z",
     "start_time": "2024-02-26T15:43:41.310293136Z"
    }
   },
   "outputs": [],
   "source": [
    "# E_MAE = meV/atom, F_MAE eV/A, S_MAE GPa\n",
    "trainer.test(model=lit_module_finetune,\n",
    "            dataloaders=test_loader\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a914e28",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-02-26T15:43:41.351939573Z",
     "start_time": "2024-02-26T15:43:41.351890380Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "path_to_csv = f'logs/M3GNet_finetuning/version_{current_version}'\n",
    "df = pd.read_csv(f'{path_to_csv}/metrics.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3134249",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-02-26T15:43:41.352064167Z",
     "start_time": "2024-02-26T15:43:41.351990960Z"
    }
   },
   "outputs": [],
   "source": [
    "# NaN to zero\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Calculate the sum of every two consecutive rows\n",
    "df = df.groupby(df.index // 2).sum()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643704df",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-20T11:11:01.699191223Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the list of loss column names\n",
    "loss_columns = [col for col in df.columns if col.startswith('val_') or col.startswith('train_')]\n",
    "\n",
    "# Create a figure and axis\n",
    "fig = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot each loss\n",
    "for loss_column in loss_columns:\n",
    "    plt.plot(df.index, np.log(df[loss_column]), label=loss_column)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc=(1.01, 0))\n",
    "plt.savefig(f'm3gnet_loss.eps', dpi=dpi, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9d85d562",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T09:19:59.496787113Z",
     "start_time": "2024-02-20T09:19:59.491695716Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(0.0135606033727526, 0.0874462649226188, 0.0)"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['val_Energy_MAE'].iloc[-2], df['val_Force_MAE'].iloc[-2], df['val_Stress_MAE'].iloc[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bc01f877",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T09:20:01.235372917Z",
     "start_time": "2024-02-20T09:20:01.228383929Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(0.0, 0.0, 0.0)"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['val_Energy_MAE'].iloc[-1], df['val_Force_MAE'].iloc[-1], df['val_Stress_MAE'].iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e76beb",
   "metadata": {},
   "source": [
    "# Cleanup the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7147751",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T09:04:42.514729694Z",
     "start_time": "2024-02-20T09:04:42.507719078Z"
    }
   },
   "outputs": [],
   "source": [
    "# This code just performs cleanup for this notebook from temporal files\n",
    "\n",
    "patterns = ['dgl_graph*.bin', 'dgl_line_graph*.bin', 'state_attr*.pt', 'labels*.json']\n",
    "for pattern in patterns:\n",
    "    files = glob.glob(pattern)\n",
    "    for file in files:\n",
    "        try:\n",
    "            os.remove(file)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "#shutil.rmtree('logs')\n",
    "#shutil.rmtree('trained_model')\n",
    "#shutil.rmtree('finetuned_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
